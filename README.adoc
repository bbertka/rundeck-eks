= Rundeck Pro Workshop with AWS EKS Kubernetes

Adapted from: https://github.com/rundeck/docker-zoo/tree/master/kubernetes

Note: currently this Readme is being refactored from original docker-zoo repo which is shown here for reference:

image:images/rundeck.png[]

== About
This workshop is a walkthrough of deploying link:https://www.rundeck.com/enterprise[Rundeck Pro].  After working through this lab you will have a web accessable deployment of Rundeck running on Amazon EKS Kubernetes, backed by MySQL DB and EFS. This is a workable architecture that can easily be used as a basis for deploying a fully HA production Rundeck cluster with further consideration for cluster scaling and object storage.

* link:https://github.com/bbertka/rundeck-eks#part-1---preparing-for-installation[Part 1 - Install Readiness]
* link:https://github.com/bbertka/rundeck-eks#part-2---deploy-an-amazon-eks-cluster[Part 2 - Deploy EKS Cluster]
* link:https://github.com/bbertka/rundeck-eks#part-3---setup-cluster-ingress[Part 3 - Setup ALB Ingress for your Cluster]
* link:https://github.com/bbertka/rundeck-eks#part-4---set-up-persistent-file-system[Part 4 - Setup EFS for file persistence]
* link:https://github.com/bbertka/rundeck-eks#part-5---deploy-rundeck-volumes[Part 5 - Deploy Rundeck Storage & MySQL DB Volumes]
* link:https://github.com/bbertka/rundeck-eks#part-6---deploy-mysql[Part 6 - Deploy MySQL]
* link:https://github.com/bbertka/rundeck-eks#part-7---deploy-rundeck[Part 7 - Deploy Rundeck]
* link:https://github.com/bbertka/rundeck-eks#part-8---connect-to-rundeck[Part 8 - Connect to Rundeck]


== Assumptions
* You already have an AWS account with typical permissions to create EC2 resources, EKS clusters, EFS file systems
* A rundeck pro license is currently issued to you, or you expect to receive one while completing this workshop.

[#preparing-for-installation]
= Part 1 - Preparing for Installation

== Setup AWS CLI
. Download and Install AWS CLI v2. 
+
----
$ curl "https://awscli.amazonaws.com/AWSCLIV2.pkg" -o "AWSCLIV2.pkg"
sudo installer -pkg AWSCLIV2.pkg -target /
----
. Check for successful installation
+
----
$ aws --version
aws-cli/2.0.38 Python/3.7.4 Darwin/17.7.0 exe/x86_64
----

. Configure your AWS credentials for your preferred region
+
----
$ aws configure
AWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE
AWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
Default region name [None]: us-east-2
Default output format [None]: json
----

== Setup Amazon EKS CLI - eksctl
. If using a Mac, we are assuming you have homebrew installed, otherwise please install it now
+
----
$ /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)"
----
. Next, install the Weavworks tap
+
----
$ brew tap weaveworks/tap
----

. Finally, install eksctl
+
----
$ brew install weaveworks/tap/eksctl
---- 

. Check for successful installation
+
----
$ eksctl version
0.25.0
----

== Setup kubectl & HELM
. Download and Install kubectl
+
----
$ curl -LO "https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/darwin/amd64/kubectl"
$ chmod +x ./kubectl
$ sudo mv ./kubectl /usr/local/bin/kubectl
----
. Check for successful installation
+
----
$ kubectl version --client
    Client Version: version.Info { 
    Major:"1",
    Minor:"18",
    GitVersion:"v1.18.4",
    GitCommit:"c96aede7b5205121079932896c4ad89bb93260af",
    GitTreeState:"clean",
    BuildDate:"2020-06-17T11:41:22Z",
    GoVersion:"go1.13.9",
    Compiler:"gc", 
    Platform:"darwin/amd64"
    }
----
. Install HELM
+
----
brew install helm
----

[#deploy-an-amazon-eks-cluster]
= Part 2 - Deploy an Amazon EKS Cluster
In order to deploy the Docker image for Rundeck Pro, we need an EKS cluster to work with.  There are two options for us for backing an EKS cluster, Fargate, or your own managed VPC nodes.  Since Rundeck Pro needs file persistence in order to write logs and configuration data, we are opting for a managed EC2 backend deployment to allow persistence with EFS.

 Download or copy your AWS public SSH key to your current directory and make sure the permissions are set securely.
+
----
$ cp ~/eks.pub . && chmod 600 eks.pub
----

. Create an EKS cluster in your region, with a node size of your choice and using your key, for example: 
+
----
$ eksctl create cluster \
 --name code-server \
 --version 1.17 \
 --region us-east-2 \
 --nodegroup-name linux-nodes \
 --node-type t3.small \
 --nodes 3 \
 --nodes-min 1 \
 --nodes-max 3 \
 --ssh-access \
 --ssh-public-key eks.pub \
 --managed
----

. After 10-15 minutes, test that Kubectl has been configured and see the ready status of your cluster nodes. 
+ 
----
$ kubectl get nodes
NAME                                           STATUS   ROLES    AGE     VERSION
ip-192-168-10-182.us-east-2.compute.internal   Ready    <none>   3m46s   v1.17.9-eks-4c6976
ip-192-168-58-121.us-east-2.compute.internal   Ready    <none>   3m49s   v1.17.9-eks-4c6976
ip-192-168-74-214.us-east-2.compute.internal   Ready    <none>   3m49s   v1.17.9-eks-4c6976
----
 Download or copy your AWS public SSH key to your current directory and make sure the permissions are set securely.
+
----
$ cp ~/eks.pub . && chmod 600 eks.pub
----

. Create an EKS cluster in your region, with a node size capable of running Rundeck Pro's link:https://docs.rundeck.com/docs/administration/install/system-requirements.html[recommended resources]
+
----
$ eksctl create cluster \
 --name rundeckpro \
 --version 1.18 \
 --region us-east-2 \
 --nodegroup-name linux-nodes \
 --node-type t2.medium \
 --nodes 1 \
 --nodes-min 1 \
 --nodes-max 1 \
 --ssh-access \
 --ssh-public-key eks.pub \
 --managed
----

. After 10-15 minutes, test that Kubectl has been configured and see the ready status of your cluster nodes.
+ 
----
$ kubectl get nodes
NAME                                           STATUS   ROLES    AGE     VERSION
ip-192-168-10-182.us-east-2.compute.internal   Ready    <none>   3m46s   v1.17.9-eks-4c6976
----

. Install the AWS EFS CSI Driver for mounting EFS volumes in pods
+
----
$ kubectl apply -k "github.com/kubernetes-sigs/aws-efs-csi-driver/deploy/kubernetes/overlays/stable/ecr/?ref=release-1.0"
----

. Verifying the EFS CSI Driver
+
----
$kubectl get pod -n kube-system

NAME                                      READY   STATUS    RESTARTS   AGE
efs-csi-node-cnv8j                        3/3     Running   0          49m
----

[#setup-cluster-ingress]
= Part 3 - Setup Cluster Ingress
To have access to Rundeck via the browser, we need to load balance to the running container within Kubernetes. To do this we create an Application Load Balancer.  Fortunetly, AWS provides an ALB controller to make this easy for us which integrates nicely with EKS.

. Create an IAM OIDC provider and associate it with your cluster
+
----
$ eksctl utils associate-iam-oidc-provider \
    --region us-east-2 \
    --cluster rundeckpro \
    --approve
----

. Download an IAM policy for the ALB Ingress Controller pod that allows it to make calls to AWS APIs on your behalf
+
----
curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/iam-policy.json
----

. Create an IAM policy called ALBIngressControllerIAMPolicy using the policy downloaded in the previous step.  Take note of the ARN policy string that was created for a following step.
+
----
$ aws iam create-policy \
    --policy-name ALBIngressControllerIAMPolicy \
    --policy-document file://iam-policy.json
----

. Create a Kubernetes service account named alb-ingress-controller in the kube-system namespace, a cluster role, and a cluster role binding for the ALB Ingress Controller to use with the following command
+
----
kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/rbac-role.yaml
----

. Create an IAM role for the ALB Ingress Controller and attach the role to the service account created in the previous step. The command that follows only works for clusters that were created with eksctl.  Note use your ARN from the previous step.
+
----
eksctl create iamserviceaccount \
    --region us-east-2 \
    --name alb-ingress-controller \
    --namespace kube-system \
    --cluster rundeckpro \
    --attach-policy-arn arn:aws:iam::111122223333:policy/ALBIngressControllerIAMPolicy \
    --override-existing-serviceaccounts \
    --approve
----

. Deploy your ALB Controller, note that initially it will error until the subsequent steps
+
----
$ kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.8/docs/examples/alb-ingress-controller.yaml
----

. Edit the ALB Deployment with your cluster name, your EKS VPC, and region
+
----
kubectl edit deployment.apps/alb-ingress-controller -n kube-system

...
    spec:
      containers:
      - args:
        - --ingress-class=alb
        - --cluster-name=rundeckpro
        - --aws-vpc-id=vpc-03468a8157edca5bd
        - --aws-region=us-east-2
----

. Confirm that the ALB Ingress Controller is running with the following command.
+
----
$ kubectl get pods -n kube-system
NAME                                      READY   STATUS    RESTARTS   AGE
alb-ingress-controller-646d767ccf-4h624   1/1     Running   0          12s
----


[#set-up-persistent-file-system]
= Part 4 - Set up Persistent File System

. Get your VPC ID for your cluster (again)
+
----
$ aws eks describe-cluster --name rundeckpro --query "cluster.resourcesVpcConfig.vpcId" --output text
vpc-015b916167f38076a
----

. Locate the CIDR range for your cluster
+
----
$ aws ec2 describe-vpcs --vpc-ids vpc-015b916167f38076a --query "Vpcs[].CidrBlock" --output text
192.168.0.0/16
----

. Create an Amazon EFS file system for your Amazon EKS cluster (link:https://docs.aws.amazon.com/eks/latest/userguide/efs-csi.html[(Steps 3 & 4 on AWS Docs)]

. Verify your File system ID 
+
----
$ aws efs describe-file-systems --query "FileSystems[*].FileSystemId" --output text
fs-41dd9839
----

. The container runs as user 'rundeck' with uid=1000(rundeck) gid=0(root) groups=0(root),27(sudo), and this impacts writing to the EFS volume. Create an EFS Application access point for the 'rundeck' user to write to our file mount. Make note of the "AccessPointId" for each execution of the command for use in the PV deployment.
+
----
$ aws efs create-access-point --file-system-id fs-41dd9839  --root-directory "Path=/var/lib/mysql"
$ aws efs create-access-point --file-system-id fs-41dd9839 --posix-user Uid=1000,Gid=0 --root-directory "Path=/home/rundeck/server/logs, CreationInfo={OwnerUid=1000,OwnerGid=0,Permissions=774}"
$ aws efs create-access-point --file-system-id fs-41dd9839 --posix-user Uid=1000,Gid=0 --root-directory "Path=/home/rundeck/server/data, CreationInfo={OwnerUid=1000,OwnerGid=0,Permissions=774}"
----

. Create a file systems access point for the MySQL container.  Make note of the "AccessPointId" for use in the PV deployment.
+
----
$ aws efs create-access-point --file-system-id fs-41dd9839  --root-directory "Path=/var/lib/mysql"
----

. Deploy the StorageClass
+
----
$ kubectl apply -f efs-storageclass.yml
storageclass.storage.k8s.io/efs-sc created
----

. View Storage Classes
+
----
$ kubectl get storageclass -n code-server
NAME            PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
efs-sc          efs.csi.aws.com         Delete          Immediate              false                  13m
gp2 (default)   kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                  4h26m
----

[#deploy-rundeck-volumes]
= Part 5 - Deploy Rundeck Storage & MySQL DB Volumes

. Update the rundeck-pv.yml csi:volumeHandles with your coresponding EFS Filesystem ID and EFS Accesspoint IDs, RUNDECK_EFS_ID::RUNDECK_EFS_ACCESSPOINT_DATA, RUNDECK_EFS_ID::RUNDECK_EFS_ACCESSPOINT_LOGS:
+
----
apiVersion: "v1"
kind: "PersistentVolume"
metadata:
  name: "rundeck-pv-data"
spec:
  capacity:
    storage: "5Gi"
  accessModes:
    - "ReadWriteMany"
  persistentVolumeReclaimPolicy: Retain
  storageClassName: efs-sc
  csi:
    driver: efs.csi.aws.com
    volumeHandle: fs-41dd9839::fsap-06c7a7f26e8436dc8

...

apiVersion: "v1"
kind: "PersistentVolume"
metadata:
  name: "rundeck-pv-logs"
spec:
  capacity:
    storage: "5Gi"
  accessModes:
    - "ReadWriteMany"
  persistentVolumeReclaimPolicy: Retain
  storageClassName: efs-sc
  csi:
    driver: efs.csi.aws.com
    volumeHandle: fs-41dd9839::fsap-0df507dac9b21a7a4

...

----

. Deploy the Rundeck PV and PVCs
+
----
$ kubectl apply -f rundeck-pv.yml
persistentvolume/rundeck-pv-data created
persistentvolumeclaim/rundeck-pv-claim-data created
persistentvolume/rundeck-pv-logs created
persistentvolumeclaim/rundeck-pv-claim-logs created
----


. Deploy the MySQL PV and PVCs
+
----
$ kubectl apply -f mysql-pv.yml
persistentvolume/mysql-pv created
persistentvolumeclaim/mysql-pv-claim created
----

. Check the status of your Volumes and Claims
+
----
$ kubectl get pv

NAME               CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                            STORAGECLASS   REASON   AGE
rundeck-pv-data    5Gi        RWX            Retain           Bound    default/rundeck-pv-claim-data    efs-sc                  2d14h
rundeck-pv-logs    5Gi        RWX            Retain           Bound    default/rundeck-pv-claim-logs    efs-sc                  2d14h
rundeck-pv-mysql   3Gi        RWX            Retain           Bound    default/rundeck-pv-claim-mysql   efs-sc                  2d19h

$ kubectl get pvc
NAME                     STATUS   VOLUME             CAPACITY   ACCESS MODES   STORAGECLASS   AGE
rundeck-pv-claim-data    Bound    rundeck-pv-data    5Gi        RWX            efs-sc         2d14h
rundeck-pv-claim-logs    Bound    rundeck-pv-logs    5Gi        RWX            efs-sc         2d14h
rundeck-pv-claim-mysql   Bound    rundeck-pv-mysql   3Gi        RWX            efs-sc         2d19h
----


[#deploy-mysql]
= Part 6 - Deploy MySQL and DB Secrets

. Create MySQL Database Password
+
----
$ echo -n 'rundeck123.' > ./password
$ kubectl create secret generic mysql-rundeckuser --from-file=./password
----

. Deploy MySQL
+
----
$ kubectl apply -f mysql-deployment.yaml
----

[#deploy-rundeck]
= Part 7 - Deploy Rundeck with Default ACL

. Create Default Rundeck ACL from secrets
+
----
$ kubectl create secret generic rundeckpro-admin-acl --from-file=./data/admin-role.aclpolicy
----

. Deploy Rundeck
+
----
$ kubectl apply -f rundeck-deployment.yaml
----

. Create a service for connecting to our container on its exposed port
+
----
$ kubectl apply -f rundeck-service.yml
service/service-rundeck created
----

. If all goes well you should see output as such:
+
----
$ kubectl get all

NAME                              READY   STATUS    RESTARTS   AGE
pod/mysql-7575f75b59-wkh48        1/1     Running   0          2d14h
pod/rundeckpro-794cd8c786-dmrcw   1/1     Running   0          2d14h

NAME                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
service/kubernetes        ClusterIP   10.100.0.1      <none>        443/TCP        3d3h
service/mysql             ClusterIP   10.100.214.8    <none>        3306/TCP       2d23h
service/service-rundeck   NodePort    10.100.199.86   <none>        80:31983/TCP   3d1h

NAME                         READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/mysql        1/1     1            1           2d20h
deployment.apps/rundeckpro   1/1     1            1           2d14h

NAME                                    DESIRED   CURRENT   READY   AGE
replicaset.apps/mysql-7575f75b59        1         1         1       2d20h
replicaset.apps/rundeckpro-794cd8c786   1         1         1       2d14h

----


[#connect-to-rundeck]
= Part 8 - Connect to Rundeck
Now that your Rundeck deployment is sucessfully running on EKS with an EFS backed persistent volume, we need to create the ALB ingress we will use to connect to the instance in our browser.

. Create the ALB Ingress 
+
----
$ kubectl apply -f rundeck-ingress.yml
ingress.extensions/rundeck-ingress created
----

. Obtain the ALB Ingress address
+
----
$ kubectl get ingress
NAME              CLASS    HOSTS   ADDRESS                                                                 PORTS   AGE
rundeck-ingress   <none>   *       58c8349a-default-rundeckin-89de-245843308.us-east-2.elb.amazonaws.com   80      3d1h
----

After a few minutes, open the browser and connect to your instance on the ALB address, if prompted for a password, use the default for admin.

image:images/login.png[]

image:images/rundeck.png[]
